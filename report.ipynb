{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Diversity Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the contemporary word2vec embeddings\n",
    "\n",
    "sample row of the file content. The word `und` is in the first position, followed by a 100-dim vector:\n",
    "\n",
    "```\n",
    "b'und -0.009612 -0.303818 0.250220 -0.162636 0.330843 -0.134072 0.016383 -0.056808 -0.041266 0.150668 0.294202 0.367427 -0.034287 -0.088949 -0.046509 0.223693 0.078327 -0.077805 0.323635 0.163179 0.079397 0.131013 0.340137 -0.292826 0.143529 -0.438839 -0.014688 0.040657 0.211812 -0.170457 0.050286 -0.034324 -0.430390 -0.136177 -0.201986 0.086426 0.219023 0.061027 -0.023832 -0.151074 0.349942 0.044963 0.149280 -0.123253 -0.099598 0.575802 -0.236324 0.234755 0.046196 -0.137115 -0.148775 -0.002521 0.141876 0.095302 -0.065125 -0.111793 -0.116030 0.028325 -0.033144 -0.088152 0.226231 -0.027581 0.088669 0.093620 -0.022539 0.294421 0.259529 -0.181482 0.366258 -0.123547 0.188571 -0.222391 0.236522 -0.233157 -0.095013 -0.219233 0.031550 0.043582 -0.077254 -0.188064 0.180374 0.115670 0.044973 0.001493 0.053256 0.241886 -0.130774 -0.096853 0.015853 -0.010810 0.016728 0.489672 0.174399 0.255024 -0.351024 0.201144 -0.165457 -0.078471 0.016386 -0.013385 \\n'\n",
    "```\n",
    "\n",
    "The unzipped file has 4.6 GB, which requires a bit of optimization when reading it.\n",
    "\n",
    "To be consistent with the historic word embeddings, we are going to store the content separated into a 1d vector for the words and a 2d vector for the word embeddings.\n",
    "\n",
    "Trying to read the file to python arrays and later casting them to numpy arrays resulted in a crash due to low memory.\n",
    "\n",
    "So we decided to choose a two pass approach, where we iterate the document twice. In the first pass, we get the now number of the file. We then initialize empty numpy arrays and fill them during a second pass.\n",
    "\n",
    "Given the precision of the numbers (eg -0.009612), we choose to initialize the array as float32. With the 4946997 rows of 100-dim vectors, this results in 1,979 MB (compared to 3,958 MB for float64). The first pass takes ~2min, the second pass ~4min.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
